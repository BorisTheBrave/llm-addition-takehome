17:00 - initial notes

I'm curious about Language Models Use Trigonometry to Do Addition. 

I've got a few possible research ideas.

A) I want to reprocue the result and test robustness,

B) Does a similar result work for multiplication?

I have some hypotheses for multiplication:
 * A parallel set of circuits that work like addition, but encode/decode log(a)
 * The same circuits are re-used, but with a log/exp step before after.
 * log itself may be approximated by MLP, or by relying on position encoding of tokens
 * Memorization or model is not really capable of multiplication.


C) Another question is whether you can still get step-by-step arithmetic if you ablate the "innate" ability to sum things

Step 1 is reading the paper and code...
..berifly reading https://openreview.net/pdf?id=S5wmbQc1We 
..am realizing paper only works on single tokens, which simplifies things considerably
..fitting the helix - don't fully understand, code will help
..ah, the paper also wonders about multiplication
..gemma-2-9b tokens digits separately - good model if you do want to investigate multi token streams

17:30 - scan of paper done, looking at code

Uses nnsight. I may need an account to run this, iirc nnsight is often run remotely?
Some datasets are precomputed, and not in the code - i may need to re-run or download.

I've got the python env working and am downloading the (LARGE) datasets. Stopping for now while that downloads as I started late today.

18:00

--- (elapsed: 1h)

13:30
decided not to run locally
faffing with vast.ai instance and env

14:10
Got that going, now seeing if i can run the repo code.

Notes on repo:
hexix_fitting seems to be main fit. Other files do various forms of analysis. "ab_" == ablation?

file_notes
data_addtion/gen_math/data_addition_...	gen_math	Questions and expected answers
data/helix_hss				run_hs		Gets hidden states of given token. Optionally sampled down to 500 elements (for perf?)


Starting to get things running. Plotted correctness addition_correctness.png
TODO: Maybe plot probability of correctness, rather than just binary?

still reading code figuring out how bits of it work

15:25
I think I've got a minimal subset of the code and plotting working. And I've got a decent grasp on what it's doing. 
It certainly seems to give v similar results to the original paper, so we can consider that weakly reconstructed. 
They already ran for different models, so not much point experimenting in that direction. 
I want to look at multiplication.


My thinking is:
1) Change gen_math to use multiplication answers
2) Filter to just one token answers
3) Check model accuracy
4) similar changes for features for fitting
5) re-run fits and investigate?

15:39
Plotted multiplication_correctness.png

Gonna take a detour to plot correctness probability.
Plotted multiplicaiton_correctness_prob.png, generallyshows reliability

Am thinking: paper recovered linear component as 1st pca. Should we look for log component before bothering with helix? Simiarly, should do FFT before looking for helix. There's no longer a "natural choice of bases".
Also thinking: 0 is a special case token as it doesn't have a log. Quite possibly there are separate circuits for it?

Starting to write code to find log component. Going to call it here for now.

16:30

--- (elapsed: 4h)

17:00
I've taken a break, let's keep going a bit. Still want to see if there is a straightforward log component.

I realize why PCA is used - data is too high dimensional otherwise when we only have 100 datapoints.

Getting some weird results when plotting PC1 with my own code - probably broken in some way. I think having repeat entries is probably bad.
... fixed. Can reproduce the original chart.
! But only when maxa=100. Increasing maxa to 361, where it should continue to behave
! I note that we can actually go to 520 as the break at 361 only occurs with a space prefix. (see method check_tokenization)

PCA components 3+ look like they are periodic. But component 1 and 2 are suspicious. See principal_components_layer_1.png

18:00 

--- (elapsed: 5h)

14:30

Investigating weird principal componts.

Paper explains this as a rgime change at 100 (due to digit change). Could be. Also could be PCA isn't able to see the feature properly as it is non linear.
Some eviden for this is that PC1 and PC2 seem to "swap" at digit 100, which is strange. pc1_pc2_swap.png.
Possibly PC1 encodes value of "leftmost 2 digits"?

I experimented with fitting a few PCA components to a logarithm, and couldn't find a good fit. It never really dipped down for the low numbers. fit_failure.

I expect I'm going to need to do as the paper does. Fit the full hidden state, then do an ablation study.

Spent ~15 running PCA and fit code vs logs

Fits still not great (judinging by MSE of log fit vs randomized version of the same, also by logdiff of activation patching

Testing a later layer, as log may not appear straight away? Fit doesn't seem noticably better this way - rules out domain warping with a MLP / non-linearity

Hmm, PCs do change. layer 3 and beyond includes a top component that only fires on 28, 80, 88? Cannot figure out why that's important. layer_3_components.png
Potentially could study by inspecting training strings that start with those numbers? It's robust to maxa.
I did feature steering, and found strength 35 was the highest i still got coherent sentences from. Nothing reliable, but it seemed to like punctuation, maths, law?
(see 88 investigation.ipynb)


Am considering doing FFT analysis looking for cos(log(a)) etc

17:00

--- (elapsed: 7.5h)

11:30

look at fft analysis.
Also negative - found no good fits.
Inspected PCAs. Found some plausible ones

aside: i checked that pythia model didn't have a similar weird component.

13:00 break for lunch
--- (elapsed: 9h) 

13:45

Checked whether the components themselves were good fits, adding somre more fitting code

14:30
Overlaying the best fit plots really sped up my investigation, as i could test a few different hypotheses.

I found a component of [1,360] that plausible looks cos(log(a)). But it's only one, which suggests it's a red herring (they should be in pairs, for clock arithmatic). log_vs_linear_fit.png

Then I filtered to [1, 99] like the paper, and looked at those. nothing for cos(log(a)).
! Then I returned to the log component, and realized that as as Principal components would suck out anything linear, the fits need to be vs (ax + b log(x)). Then I found an *excellent* fit for PC3, which seems pretty solid. log_linear_true_99_fit.png

Ok, so working theory is that there is no clock arithmetic for log(a). Quite possibly it's not needed when the range of numbers that gives a single token output is so much smaller?

Next I would like to validate the linear component. Going to review the existing paper/code for suggestions on how to do that.

Yes, using the fitting code actually shows logpoly_1 has a better mse than poly_1.
I want to fit poly+logpoly, like I did before...

15:46

11 hours in, i'm considering wrapping this up. I've run the intervention code, and it shows that PCA outperforms x+log(x), which is embarrsing as it has twice the parameters to tune!

My tentative conclusion is that the addition spiral is being re-used somehow. log_patching.png

15:55

--- (elapsed: 11h 10m)
