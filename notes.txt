17:00 - initial notes

I'm curious about Language Models Use Trigonometry to Do Addition. 

I've got a few possible research ideas.

A) I want to reprocue the result and test robustness,

B) Does a similar result work for multiplication?

I have some hypotheses for multiplication:
 * A parallel set of circuits that work like addition, but encode/decode log(a)
 * The same circuits are re-used, but with a log/exp step before after.
 * log itself may be approximated by MLP, or by relying on position encoding of tokens
 * Memorization or model is not really capable of multiplication.


C) Another question is whether you can still get step-by-step arithmetic if you ablate the "innate" ability to sum things

Step 1 is reading the paper and code...
..berifly reading https://openreview.net/pdf?id=S5wmbQc1We 
..am realizing paper only works on single tokens, which simplifies things considerably
..fitting the helix - don't fully understand, code will help
..ah, the paper also wonders about multiplication
..gemma-2-9b tokens digits separately - good model if you do want to investigate multi token streams

17:30 - scan of paper done, looking at code

Uses nnsight. I may need an account to run this, iirc nnsight is often run remotely?
Some datasets are precomputed, and not in the code - i may need to re-run or download.

I've got the python env working and am downloading the (LARGE) datasets. Stopping for now while that downloads as I started late today.

18:00

--- (elapsed: 1h)

13:30
decided not to run locally
faffing with vast.ai instance and env

14:10
Got that going, now seeing if i can run the repo code.

Notes on repo:
hexix_fitting seems to be main fit. Other files do various forms of analysis. "ab_" == ablation?

file_notes
data_addtion/gen_math/data_addition_...	gen_math	Questions and expected answers
data/helix_hss				run_hs		Gets hidden states of given token. Optionally sampled down to 500 elements (for perf?)


Starting to get things running. Plotted correctness addition_correctness.png
TODO: Maybe plot probability of correctness, rather than just binary?

still reading code figuring out how bits of it work

15:25
I think I've got a minimal subset of the code and plotting working. And I've got a decent grasp on what it's doing. 
It certainly seems to give v similar results to the original paper, so we can consider that weakly reconstructed. 
They already ran for different models, so not much point experimenting in that direction. 
I want to look at multiplication.


My thinking is:
1) Change gen_math to use multiplication answers
2) Filter to just one token answers
3) Check model accuracy
4) similar changes for features for fitting
5) re-run fits and investigate?

15:39
Plotted multiplication_correctness.png

Gonna take a detour to plot correctness probability.
Plotted multiplicaiton_correctness_prob.png, generallyshows reliability

Am thinking: paper recovered linear component as 1st pca. Should we look for log component before bothering with helix? Simiarly, should do FFT before looking for helix. There's no longer a "natural choice of bases".
Also thinking: 0 is a special case token as it doesn't have a log. Quite possibly there are separate circuits for it?

Starting to write code to find log component. Going to call it here for now.

16:30

--- (elapsed: 4h)

17:00
I've taken a break, let's keep going a bit. Still want to see if there is a straightforward log component.

I realize why PCA is used - data is too high dimensional otherwise when we only have 100 datapoints.

Getting some weird results when plotting PC1 with my own code - probably broken in some way. I think having repeat entries is probably bad.
... fixed. Can reproduce the original chart.
! But only when maxa=100. Increasing maxa to 361, where it should continue to behave
! I note that we can actually go to 520 as the break at 361 only occurs with a space prefix. (see method check_tokenization)

PCA components 3+ look like they are periodic. But component 1 and 2 are suspicious. See principal_components_layer_1.png

18:00 

--- (elapsed: 5h)